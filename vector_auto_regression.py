# -*- coding: utf-8 -*-
"""Vector Auto Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P6l1_bcuqqDba_SquBmsZIyYuQC66h95

## **Import Dependencies**
"""

import os
import random
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
import statsmodels.tsa.stattools as ts

from tqdm import tqdm
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import grangercausalitytests

from keras.models import load_model
from keras.callbacks import ModelCheckpoint
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score

warnings.filterwarnings("ignore")
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

"""## **Read the Dataset**"""

data = pd.read_csv("/content/ASII.csv")

column_to_drop = ["previous", "open_price", "first_trade", "high", "low", "delisting_date"]
data = data.drop(column_to_drop, axis=1)
data.head()

"""## **Understanding the Dataset**"""

# Check the percentage of Nan in dataset
total        = data.isnull().sum().sort_values(ascending=False)
percent      = (data.isnull().sum() / len(data)).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(15)

def visualize_heatmap(data):
    """
    Visualize a heatmap of the correlation matrix for the DataFrame.

    Parameters:
    - data (pd.DataFrame): The DataFrame for which the heatmap is generated.

    Returns:
    None
    """
    # Calculate the correlation matrix
    correlation_matrix = data.corr()

    # Set the size of the figure
    plt.figure(figsize=(15, 10))

    # Display the heatmap
    sns.heatmap(correlation_matrix, cmap='coolwarm', fmt='.2f', linewidths=0.5)

    # Adjust the layout
    plt.title('Heatmap of Column Correlations')
    plt.show()

# Call the function to display the heatmap
visualize_heatmap(data)

def visualize_timeframe(data, target_column="close"):
    """
    Visualize the target column using a line chart with fill between.

    Parameters:
    - df (pd.DataFrame): The DataFrame containing the data.
    - target_column (str, optional): The column to visualize. Default is target.

    Returns:
    None
    """
    # Set the size of the figure
    plt.figure(figsize=(14, 6))

    # Create a line chart for target
    plt.plot(data.index, data[target_column], label=target_column, color='blue', linestyle='-', linewidth=1)

    # Add fill between the line and the x-axis
    plt.fill_between(data.index, data[target_column], color='blue', alpha=0.2)

    # Add labels and title
    plt.xlabel('Date', fontsize=14)
    plt.ylabel(target_column, fontsize=14)
    plt.title(f'Line Chart of {target_column} with Fill Between', fontsize=16)

    # Add a legend
    plt.legend(fontsize=12)

    # Add a grid
    plt.grid(True, linestyle='--', alpha=0.7)

    # Adjust the layout
    plt.tight_layout()
    plt.show()

# Visualize the close price
visualize_timeframe(data)

"""## **Preprocessing the Dataset**"""

# Mengonversi kolom 'date' menjadi tipe data datetime
data['date'] = pd.to_datetime(data['date'])

# Menjadikan kolom 'date' sebagai indeks
data.set_index('date', inplace=True)
data.head()

# Inisialisasi MinMaxScaler
scaler = MinMaxScaler()

# Mengambil kolom yang akan dinormalisasi
numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Melakukan normalisasi untuk kolom-kolom numerik
data[numeric_columns] = scaler.fit_transform(data[numeric_columns])
data.head()

"""## **Feature Selection**"""

# Initialize min_test_MSE and best_columns
min_test_MSE = float('inf')
best_columns = None
best_lags    = 0
target       = "close"

# Search best feature with iteration
for i in tqdm(range(1000), desc="Processing", unit="iteration"):
    try:
        # Choose random features
        selected_data = data.sample(n=random.randint(2, 2 + int(i/200)), axis=1, replace=False)
        selected_data[target] = data[target]
        data_ = selected_data.copy()

        # Sort index
        data_ = data_.sort_index(ascending=True)

        # Split data
        data_train = data_[:int(0.8*len(data_))]
        data_test  = data_[int(0.8*len(data_)):]

        # Fit VAR model to the training data
        model = VAR(data_train)
        lag_selection_VAR = model.select_order(maxlags=20 - int(i/200))

        # Get the lag order
        lag_order = lag_selection_VAR.selected_orders['aic']
        model_fitted = model.fit(lag_order)

        # Number of predicted time steps
        steps = len(data_test) + 100

        # Predict use VAR model
        forecast       = model_fitted.forecast(data_train.values, steps = steps)
        forecast_index = pd.date_range(start=data_train.index[-1], freq='B', periods=steps + 1)[1:]
        forecast_data  = pd.DataFrame(forecast, index=forecast_index, columns=data_train.columns)

        # Calculate residuals (errors) on the training data
        train_residuals = data_train[lag_order:][target] - model_fitted.fittedvalues.values[:, -1]
        test_residuals = data_test[target] - forecast_data[target]

        # Calculate Mean Squared Error (MSE) for each variable
        train_MSE = mean_squared_error(data_train[lag_order:][target], model_fitted.fittedvalues.values[:, -1])
        test_MSE = mean_squared_error(data_test[target], forecast_data[:-100][target])

        # Check if current test_MSE is the smallest
        if test_MSE < min_test_MSE:
            min_test_MSE = test_MSE
            best_columns = data_.columns.tolist()
            best_lags    = lag_order

    except Exception as e:
        i = i - 1
        print(f"\n -- An error occurred: {e}")

# Print the columns with the smallest test_MSE
print(f"\n\nBest columns with the smallest test_MSE is {best_columns} with lags {best_lags}")
print(f"Smallest test_MSE value: {min_test_MSE}")

data = data[best_columns]
data.head()

"""## **Split the Dataset**"""

data_train = data[:int(0.8*len(data))]
data_test  = data[int(0.8*len(data)):]

print("Data train shape :", data_train.shape)
print("Data test shape  :", data_test.shape)

"""## **Model Training**"""

# Fit VAR model to the training data
model = VAR(data_train)
lag_selection_VAR = model.select_order(maxlags=best_lags)
print(lag_selection_VAR.summary())

# Get the lag order
lag_order = lag_selection_VAR.selected_orders['aic']
print("Best lag order :", lag_order)
model_fitted = model.fit(lag_order)

# Number of predicted time steps
steps = len(data_test) + 100

# Predict use VAR model
forecast       = model_fitted.forecast(data_train.values, steps = steps)
forecast_index = pd.date_range(start=data_train.index[-1], freq='B', periods=steps + 1)[1:]
forecast_data  = pd.DataFrame(forecast, index=forecast_index, columns=data_train.columns)
forecast_data.head()

# Plot the results
plt.figure(figsize=(12, 3))

# Plot the training, test, and forecasted
plt.plot(data_train.index    , data_train[target]   , label='Training Data', color='green')
plt.plot(data_test.index     , data_test[target]    , label='Test Data'    , color='red')
plt.plot(forecast_data.index , forecast_data[target], label='Test Forecast', color='blue', linestyle='--')

plt.plot(data_train.index[lag_order + len(data_train[lag_order:])-100:],
         model_fitted.fittedvalues.values[len(data_train[lag_order:])-100:, -1],
         label='Train Forecast', color='blue')

plt.title('VAR Model Forecasting')
plt.xlabel('Date')
plt.ylabel(target)
plt.legend()
plt.grid(True)
plt.show()

# Performance on Training Data
plt.figure(figsize=(12, 3))
plt.plot(data_train[lag_order:].index, data_train[lag_order:]['close']  , label='Training Data' , color='green')
plt.plot(data_train[lag_order:].index, model_fitted.fittedvalues.values[:, -1], label='Train Forecast', color='blue')
plt.title('Performance on Training Data')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show()

# Calculate residuals (errors) on the training data
train_residuals = data_train[lag_order:]['close'] - model_fitted.fittedvalues.values[:, -1]
test_residuals = data_test['close'] - forecast_data['close']

# Calculate Mean Squared Error (MSE) for each variable
train_MSE = mean_squared_error(data_train[lag_order:]['close'], model_fitted.fittedvalues.values[:, -1])
test_MSE = mean_squared_error(data_test['close'], forecast_data[:-100]['close'])

# Print the MSE for each feature
print("Close Price MSE on Training Data:", train_MSE)
print("Close Price MSE on Test Data:", test_MSE)